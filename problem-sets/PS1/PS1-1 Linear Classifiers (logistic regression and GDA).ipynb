{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "## PS1-1 Linear Classifiers (logistic regression and GDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Recall the average empirical loss for logistic regression:\n",
    "\n",
    "$$\n",
    "J(\\theta) = - \\frac{1}{m} \\sum_{i = 1}^{m} y^{(i)} \\log \\big( h_\\theta (x^{(i)}) \\big) + (1 - y^{(i)}) \\log \\big( 1 - h_\\theta (x^{(i)}) \\big)\n",
    "$$,\n",
    "\n",
    "where $y^{(i)} \\in \\{ 0, 1 \\}, h_\\theta (x) = g(\\theta^T x)$ and $g(z) = 1 / (1 + e^{-z})$.\n",
    "\n",
    "Compute the gradient of $J(\\theta)$:\n",
    "\n",
    "\\begin{align*}\n",
    "\\nabla_\\theta J(\\theta) & = - \\frac{1}{m} \\sum_{i = 1}^{m} y^{(i)} \\frac{1}{h_\\theta (x^{(i)})} h_\\theta (x^{(i)}) \\big( 1 - h_\\theta (x^{(i)}) \\big) - (1 - y^{(i)}) \\frac{1}{1 - h_\\theta (x^{(i)})} h_\\theta (x^{(i)}) \\big( 1 - h_\\theta (x^{(i)}) \\big) \\\\\n",
    "                        & = - \\frac{1}{m} \\sum_{i = 1}^{m} \\big( y^{(i)} - h_\\theta (x^{(i)}) \\big) x^{(i)}\n",
    "\\end{align*}\n",
    "\n",
    "Then, the Hessian of $J(\\theta)$ is:\n",
    "\n",
    "$$H = \\nabla_\\theta^2 J(\\theta) = \\frac{1}{m} \\sum_{i = 1}^{m}  h_\\theta (x^{(i)}) \\big( 1 - h_\\theta (x^{(i)}) \\big) x^{(i)} (x^{(i)})^T$$\n",
    "\n",
    "And for any $z \\in \\mathbb{R}^n$:\n",
    "\n",
    "\\begin{align*}\n",
    "z^T H z & = \\sum_{j = 1}^{n} \\sum_{k = 1}^{n} H_{jk} z_j z_k \\\\\n",
    "        & = \\sum_{j = 1}^{n} \\sum_{k = 1}^{n} \\frac{1}{m} \\sum_{i = 1}^{m}  h_\\theta (x^{(i)}) \\big( 1 - h_\\theta (x^{(i)}) \\big) x_j^{(i)} x_k^{(i)} z_j z_k \\\\\n",
    "        & = \\frac{1}{m} \\sum_{i = 1}^{m}  h_\\theta (x^{(i)}) \\big( 1 - h_\\theta (x^{(i)}) \\big) \\sum_{j = 1}^{n} \\sum_{k = 1}^{n} z_j x_j^{(i)} x_k^{(i)} z_k \\\\\n",
    "        & = \\frac{1}{m} \\sum_{i = 1}^{m}  h_\\theta (x^{(i)}) \\big( 1 - h_\\theta (x^{(i)}) \\big) \\big( (x^{(i)})^T z \\big)^2 \\\\\n",
    "        & \\geq 0\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% code\n"
    }
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m运行具有“cs229 (Python 3.12.6)”的单元格需要ipykernel包。\n",
      "\u001b[1;31m运行以下命令，将 \"ipykernel\" 安装到 Python 环境中。\n",
      "\u001b[1;31m命令: \"c:/Windows/System32/cs229/Scripts/python.exe -m pip install ipykernel -U --force-reinstall\""
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import problem_set_1.src.util as util\n",
    "\n",
    "from problem_set_1.src.linear_model import LinearModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% code\n"
    }
   },
   "outputs": [],
   "source": [
    "ds1_training_set_path = 'data/ds1_train.csv'\n",
    "ds1_valid_set_path = 'data/ds1_valid.csv'\n",
    "ds2_training_set_path = 'data/ds2_train.csv'\n",
    "ds2_valid_set_path = 'data/ds2_valid.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% code\n"
    }
   },
   "outputs": [],
   "source": [
    "x_train, y_train = util.load_dataset(ds1_training_set_path, add_intercept=True)\n",
    "x_valid, y_valid = util.load_dataset(ds1_valid_set_path, add_intercept=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Take a look at the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% code\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.plot(x_train[y_train == 1, -2], x_train[y_train == 1, -1], 'bx', linewidth=2)\n",
    "plt.plot(x_train[y_train == 0, -2], x_train[y_train == 0, -1], 'go', linewidth=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Implement logistic regression using Newton's Method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% code\n"
    }
   },
   "outputs": [],
   "source": [
    "class LogisticRegression(LinearModel):\n",
    "    \"\"\"Logistic regression with Newton's Method as the solver.\n",
    "\n",
    "    Example usage:\n",
    "        > clf = LogisticRegression()\n",
    "        > clf.fit(x_train, y_train)\n",
    "        > clf.predict(x_eval)\n",
    "    \"\"\"\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        \"\"\"Run Newton's Method to minimize J(theta) for logistic regression.\n",
    "\n",
    "        :param x: Training example inputs. Shape (m, n).\n",
    "        :param y: Training example labels. Shape (m,).\n",
    "        \"\"\"\n",
    "\n",
    "        def h(theta, x):\n",
    "            \"\"\"Vectorized implementation of h_theta(x) = 1 / (1 + exp(-theta^T x)).\n",
    "\n",
    "            :param theta: Shape (n,).\n",
    "            :param x:     All training examples of shape (m, n).\n",
    "            :return:      The hypothesis for all training examples. Shape (m,).\n",
    "            \"\"\"\n",
    "            return 1 / (1 + np.exp(-np.dot(x, theta)))\n",
    "\n",
    "        def gradient(theta, x, y):\n",
    "            \"\"\"Vectorized implementation of the gradient of J(theta).\n",
    "\n",
    "            :param theta: Shape (n,).\n",
    "            :param x:     All training examples of shape (m, n).\n",
    "            :param y:     All labels of shape (m,).\n",
    "            :return:      The gradient of shape (n,).\n",
    "            \"\"\"\n",
    "            m, _ = x.shape\n",
    "            return -1 / m * np.dot(x.T, (y - h(theta, x)))\n",
    "\n",
    "        def hessian(theta, x):\n",
    "            \"\"\"Vectorized implementation of the Hessian of J(theta).\n",
    "\n",
    "            :param theta: Shape (n,).\n",
    "            :param x:     All training examples of shape (m, n).\n",
    "            :return:      The Hessian of shape (n, n).\n",
    "            \"\"\"\n",
    "            m, _ = x.shape\n",
    "            h_theta_x = np.reshape(h(theta, x), (-1, 1))\n",
    "            return 1 / m * np.dot(x.T, h_theta_x * (1 - h_theta_x) * x)\n",
    "\n",
    "        def next_theta(theta, x, y):\n",
    "            \"\"\"The next theta updated by Newton's Method.\n",
    "\n",
    "            :param theta: Shape (n,).\n",
    "            :return:      The updated theta of shape (n,).\n",
    "            \"\"\"\n",
    "            return theta - np.dot(np.linalg.inv(hessian(theta, x)), gradient(theta, x, y))\n",
    "\n",
    "        m, n = x.shape\n",
    "\n",
    "        # Initialize theta\n",
    "        if self.theta is None:\n",
    "            self.theta = np.zeros(n)\n",
    "\n",
    "        # Update theta using Newton's Method\n",
    "        \n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"Make a prediction given new inputs x.\n",
    "\n",
    "        :param x: Inputs of shape (m, n).\n",
    "        :return:  Outputs of shape (m,).\n",
    "        \"\"\"\n",
    "\n",
    "        return x @ self.theta >= 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Train the logistic regression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% code\n"
    }
   },
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Plot decision boundary for training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% code\n"
    }
   },
   "outputs": [],
   "source": [
    "util.plot(x_train, y_train, theta=log_reg.theta)\n",
    "print(\"Theta is: \", log_reg.theta)\n",
    "print(\"The accuracy on training set is: \", np.mean(log_reg.predict(x_train) == y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Plot decision boundary for validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% code\n"
    }
   },
   "outputs": [],
   "source": [
    "util.plot(x_valid, y_valid, log_reg.theta)\n",
    "print(\"The accuracy on validation set is: \", np.mean(log_reg.predict(x_valid) == y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### (c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\\begin{align*}\n",
    "p(y = 1 \\ \\vert \\ x; \\ \\phi, \\mu_0, \\mu_1, \\Sigma) & = \\frac{p(x\\ \\vert \\ y = 1; \\ \\mu_0, \\mu_1, \\Sigma) \\ p(y = 1; \\ \\phi)}{p(x\\ \\vert \\ y = 1; \\ \\mu_0, \\mu_1, \\Sigma) \\ p(y = 1; \\ \\phi) + p(x\\ \\vert \\ y = 0; \\ \\mu_0, \\mu_1, \\Sigma) \\ p(y = 0; \\ \\phi)} \\\\\n",
    "                                                   & = 1 / (1 + \\frac{p(x\\ \\vert \\ y = 0; \\ \\mu_0, \\mu_1, \\Sigma) \\ p(y = 0; \\ \\phi)}{p(x\\ \\vert \\ y = 1; \\ \\mu_0, \\mu_1, \\Sigma) \\ p(y = 1; \\ \\phi)}) \\\\\n",
    "                                                   & = 1 / (1 + \\exp \\big( \\frac{1}{2} (x - \\mu_1)^T \\Sigma^{-1} (x - \\mu_1) - \\frac{1}{2} (x - \\mu_0)^T \\Sigma^{-1} (x - \\mu_0) \\big) \\frac{1 - \\phi}{\\phi}) \\\\\n",
    "                                                   & = 1 / (1 + \\exp \\big( - \\big( (\\mu_1 - \\mu_0)^T \\Sigma^{-1} x + (\\mu_0^T \\Sigma^{-1} \\mu_0 - \\mu_1^T \\Sigma^{-1} \\mu_1 - \\log \\frac{1 - \\phi}{\\phi}) \\big) \\big) ) \\\\\n",
    "                                                   & = 1 / (1 + \\exp \\big( -(\\theta^T x + \\theta_0) \\big))\n",
    "\\end{align*}\n",
    "\n",
    "where $\\theta = \\Sigma^{-1} (\\mu_1 - \\mu_0)$ and $\\theta_0 = \\mu_0^T \\Sigma^{-1} \\mu_0 - \\mu_1^T \\Sigma^{-1} \\mu_1 - \\log \\frac{1 - \\phi}{\\phi}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### (d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To compute $\\phi$, $\\mu_0$ and $\\mu_1$, recall the log-likelihood:\n",
    "\n",
    "\\begin{align*}\n",
    "\\ell(\\phi, \\mu_0, \\mu_1, \\Sigma) & = \\log \\prod_{i = 1}^{m} p(x^{(i)}, y^{(i)}; \\ \\phi, \\mu_0, \\mu_1, \\Sigma) \\\\\n",
    "                                 & = \\log \\prod_{i = 1}^{m} p(x^{(i)} \\ \\vert \\ y^{(i)}; \\ \\mu_0, \\mu_1, \\Sigma) \\ p(y^{(i)}; \\ \\phi) \\\\\n",
    "                                 & = \\log \\prod_{i = 1}^{m} \\big( p(x^{(i)} \\ \\vert \\ y^{(i)} = 1; \\ \\mu_0, \\mu_1, \\Sigma) \\ p(y^{(i)} = 1; \\ \\phi) \\big) ^{1 \\{ y^{(i)} = 1 \\}} \\big( p(x^{(i)} \\ \\vert \\ y^{(i)} = 0; \\ \\mu_0, \\mu_1, \\Sigma) \\ p(y^{(i)} = 0; \\ \\phi) \\big) ^{1 \\{ y^{(i)} = 0 \\}} \\\\\n",
    "                                 & = \\sum_{i = 1}^{m} 1 \\{ y^{(i)} = 1 \\} \\big( -\\frac{1}{2} (x^{(i)} - \\mu_1)^T \\Sigma^{-1} (x^{(i)} - \\mu_1) + \\log \\phi \\big) + \\sum_{i = 1}^{m} 1 \\{ y^{(i)} = 0 \\} \\big( -\\frac{1}{2} (x^{(i)} - \\mu_0)^T \\Sigma^{-1} (x^{(i)} - \\mu_0) + \\log (1 - \\phi) \\big) + C\n",
    "\\end{align*}\n",
    "\n",
    "where $C$ does not contain $\\phi$, $\\mu_0$ or $\\mu_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Take derivative of $\\ell$ w.r.t $\\phi$ and set to 0:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial}{\\partial \\phi} \\ell(\\phi, \\mu_0, \\mu_1, \\Sigma) & = \\sum_{i = 1}^{m} 1 \\{ y^{(i)} = 1 \\} \\frac{1}{\\phi} + (m - \\sum_{i = 1}^{m} 1 \\{ y^{(i)} = 1 \\}) \\frac{1}{1 - \\phi} \\\\\n",
    "                                                                & = 0\n",
    "\\end{align*}\n",
    "\n",
    "We have $\\phi = \\frac{1}{m} \\sum_{i = 1}^{m} 1 \\{ y^{(i)} = 1 \\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Also, take derivative of $\\ell$ w.r.t $\\mu_0$ and set to 0:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial}{\\partial \\mu_0} \\ell(\\phi, \\mu_0, \\mu_1, \\Sigma) & = \\sum_{i = 1}^{m} 1 \\{ y^{(i)} = 0 \\} \\Sigma^{-1} (x^{(i)} - \\mu_0) \\\\\n",
    "                                                                 & = 0\n",
    "\\end{align*}\n",
    "\n",
    "We can easily obtain that $\\mu_0 = \\sum_{i = 1}^{m} 1 \\{ y^{(i)} = 0\\} x^{(i)} / \\sum_{i = 1}^{m} 1 \\{ y^{(i)} = 0\\}$. Similarly, $\\mu_1 = \\sum_{i = 1}^{m} 1 \\{ y^{(i)} = 1\\} x^{(i)} / \\sum_{i = 1}^{m} 1 \\{ y^{(i)} = 1\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To compute $\\Sigma$, we need to simplify $\\ell$ while maintaining $\\Sigma$:\n",
    "\n",
    "\\begin{align*}\n",
    "\\ell(\\phi, \\mu_0, \\mu_1, \\Sigma) & = \\log \\prod_{i = 1}^{m} p(x^{(i)} \\ \\vert \\ y^{(i)}; \\ \\mu_0, \\mu_1, \\Sigma) \\ p(y^{(i)}; \\ \\phi) \\\\\n",
    "                                 & = - \\frac{m}{2} \\log \\vert \\Sigma \\vert - \\frac{1}{2} \\sum_{i = 1}^{m} (x^{(i)} - \\mu_{y^{(i)}})^T \\Sigma^{-1} (x^{(i)} - \\mu_{y^{(i)}}) + C \\\\\n",
    "                                 & = - \\frac{m}{2} \\log \\vert \\Sigma \\vert - \\frac{1}{2} \\sum_{i = 1}^{m} \\operatorname{tr} \\big( (x^{(i)} - \\mu_{y^{(i)}})^T \\Sigma^{-1} (x^{(i)} - \\mu_{y^{(i)}}) \\big) + C \\\\\n",
    "                                 & = - \\frac{m}{2} \\log \\vert \\Sigma \\vert - \\frac{1}{2} \\sum_{i = 1}^{m} \\operatorname{tr} \\big( \\Sigma^{-1} (x^{(i)} - \\mu_{y^{(i)}}) (x^{(i)} - \\mu_{y^{(i)}})^T \\big) + C\n",
    "\\end{align*}\n",
    "\n",
    "Since $n = 1$, i.e. $\\vert \\Sigma \\vert = \\sigma^2$, by taking derivative of $\\ell$ w.r.t $\\Sigma$ and set to 0:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial}{\\partial \\Sigma} \\ell(\\phi, \\mu_0, \\mu_1, \\Sigma) & = - \\frac{m}{2 \\Sigma} + \\frac{1}{2} \\sum_{i = 1}^{m} (x^{(i)} - \\mu_{y^{(i)}}) (x^{(i)} - \\mu_{y^{(i)}})^T \\Sigma^{-2} \\\\\n",
    "                                                                  & = 0\n",
    "\\end{align*}\n",
    "\n",
    "We have: $\\Sigma = \\frac{1}{m} \\sum_{i = 1}^{m} (x^{(i)} - \\mu_{y^{(i)}}) (x^{(i)} - \\mu_{y^{(i)}})^T$.\n",
    "\n",
    "In fact, even if $n \\neq 1$, this maximum likelihood estimate still holds. Recall that:\n",
    "\n",
    "\\begin{align*}\n",
    "\\det (A^{-1}) & = \\frac{1}{\\det (A)} \\\\\n",
    "\\frac{\\partial}{\\partial A} \\log \\vert A \\vert & = A^{-T}\n",
    "\\end{align*}\n",
    "\n",
    "Simplify $\\ell$ w.r.t $\\Sigma^{-1}$:\n",
    "\n",
    "\\begin{align*}\n",
    "\\ell(\\phi, \\mu_0, \\mu_1, \\Sigma) & = - \\frac{m}{2} \\log \\vert \\Sigma \\vert - \\frac{1}{2} \\sum_{i = 1}^{m} (x^{(i)} - \\mu_{y^{(i)}})^T \\Sigma^{-1} (x^{(i)} - \\mu_{y^{(i)}}) + C \\\\\n",
    "                                 & = \\frac{m}{2} \\log \\vert \\Sigma^{-1} \\vert - \\frac{1}{2} \\sum_{i = 1}^{m} \\Sigma^{-1} (x^{(i)} - \\mu_{y^{(i)}}) (x^{(i)} - \\mu_{y^{(i)}})^T + C\n",
    "\\end{align*}\n",
    "\n",
    "We can derive the same estimate by solving:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial}{\\partial \\Sigma^{-1}} \\ell(\\phi, \\mu_0, \\mu_1, \\Sigma) & = \\frac{m}{2} \\Sigma - \\frac{1}{2} \\sum_{i = 1}^{m} (x^{(i)} - \\mu_{y^{(i)}}) (x^{(i)} - \\mu_{y^{(i)}})^T \\\\\n",
    "                                                                       & = 0\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### (e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Implement GDA model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% code\n"
    }
   },
   "outputs": [],
   "source": [
    "class GDA(LinearModel):\n",
    "    \"\"\"Gaussian Discriminant Analysis.\n",
    "\n",
    "    Example usage:\n",
    "        > clf = GDA()\n",
    "        > clf.fit(x_train, y_train)\n",
    "        > clf.predict(x_eval)\n",
    "    \"\"\"\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        \"\"\"Fit a GDA model to training set given by x and y.\n",
    "\n",
    "        :param x: Training example inputs. Shape (m, n).\n",
    "        :param y: Training example labels. Shape (m,).\n",
    "        \"\"\"\n",
    "\n",
    "        m, n = x.shape\n",
    "\n",
    "        phi = np.sum(y) / m\n",
    "        mu_0 = np.dot(x.T, 1 - y) / np.sum(1 - y)\n",
    "        mu_1 = np.dot(x.T, y) / np.sum(y)\n",
    "\n",
    "        # Reshape y to compute pairwise product with mu\n",
    "        y_reshaped = np.reshape(y, (m, -1))\n",
    "\n",
    "        # Matrix comprises mu_0 and mu_1 based on the value of y. Shape(m, n)\n",
    "        mu_x = y_reshaped * mu_1 + (1 - y_reshaped) * mu_0\n",
    "\n",
    "        x_centered = x - mu_x\n",
    "\n",
    "        sigma = np.dot(x_centered.T, x_centered) / m\n",
    "        sigma_inv = np.linalg.inv(sigma)\n",
    "\n",
    "        # Compute theta and theta_0 according to the conclusion from part (c)\n",
    "        theta = np.dot(sigma_inv, mu_1 - mu_0)\n",
    "        theta_0 = 1 / 2 * mu_0 @ sigma_inv @ mu_0 - 1 / 2 * mu_1 @ sigma_inv @ mu_1 - np.log((1 - phi) / phi)\n",
    "\n",
    "        self.theta = np.insert(theta, 0, theta_0)\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"Make a prediction given new inputs x.\n",
    "\n",
    "        :param x: Inputs of shape (m, n).\n",
    "        :return:  Outputs of shape (m,).\n",
    "        \"\"\"\n",
    "\n",
    "        # Add x_0 = 1 convention to make predictions using theta^T x >= 0\n",
    "        return util.add_intercept(x) @ self.theta >= 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### (f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Rewrite `plot` function to allow a second decision boundary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% code\n"
    }
   },
   "outputs": [],
   "source": [
    "def plot(x, y, theta_1, legend_1=None, theta_2=None, legend_2=None, title=None, correction=1.0):\n",
    "    # Plot dataset\n",
    "    plt.figure()\n",
    "    plt.plot(x[y == 1, -2], x[y == 1, -1], 'bx', linewidth=2)\n",
    "    plt.plot(x[y == 0, -2], x[y == 0, -1], 'go', linewidth=2)\n",
    "\n",
    "    # Plot decision boundary (found by solving for theta_1^T x = 0)\n",
    "    x1 = np.arange(min(x[:, -2]), max(x[:, -2]), 0.01)\n",
    "    x2 = -(theta_1[0] / theta_1[2] * correction + theta_1[1] / theta_1[2] * x1)\n",
    "    plt.plot(x1, x2, c='red', label=legend_1, linewidth=2)\n",
    "\n",
    "    # Plot decision boundary (found by solving for theta_2^T x = 0)\n",
    "    if theta_2 is not None:\n",
    "        x1 = np.arange(min(x[:, -2]), max(x[:, -2]), 0.01)\n",
    "        x2 = -(theta_2[0] / theta_1[2] * correction + theta_2[1] / theta_2[2] * x1)\n",
    "        plt.plot(x1, x2, c='black', label=legend_2, linewidth=2)\n",
    "\n",
    "    # Add labels, legend and title\n",
    "    plt.xlabel('x1')\n",
    "    plt.ylabel('x2')\n",
    "    if legend_1 is not None or legend_2 is not None:\n",
    "        plt.legend(loc=\"upper left\")\n",
    "    if title is not None:\n",
    "        plt.suptitle(title, fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Load dataset 1 and drop the \"$x_0 = 1$\" convention:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% code\n"
    }
   },
   "outputs": [],
   "source": [
    "x_train, y_train = util.load_dataset(ds1_training_set_path)\n",
    "x_valid, y_valid = util.load_dataset(ds1_valid_set_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Train GDA model using dataset 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% code\n"
    }
   },
   "outputs": [],
   "source": [
    "gda = GDA()\n",
    "gda.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Plot decision boundaries found by logistic regression and GDA on validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% code\n"
    }
   },
   "outputs": [],
   "source": [
    "plot(x_valid, y_valid, theta_1=log_reg.theta, legend_1='logistic regression', theta_2=gda.theta, legend_2='GDA', title='Validation Set 1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "GDA performs worse than logistic regression on dataset 1 as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### (g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Repeat the steps in part (f) for dataset 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% code\n"
    }
   },
   "outputs": [],
   "source": [
    "# Train logistic regression\n",
    "x_train, y_train = util.load_dataset(ds2_training_set_path, add_intercept=True)\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(x_train, y_train)\n",
    "\n",
    "# Train GDA\n",
    "x_train, y_train = util.load_dataset(ds2_training_set_path)\n",
    "gda = GDA()\n",
    "gda.fit(x_train, y_train)\n",
    "\n",
    "# Plot decision boundaries on training set 2\n",
    "plot(x_train, y_train, theta_1=log_reg.theta, legend_1='logistic regression', theta_2=gda.theta, legend_2='GDA', title='Training Set 2')\n",
    "\n",
    "# Plot decision boundaries on validation set 2\n",
    "x_valid, y_valid = util.load_dataset(ds2_valid_set_path)\n",
    "plot(x_valid, y_valid, theta_1=log_reg.theta, legend_1='logistic regression', theta_2=gda.theta, legend_2='GDA', title='Validation Set 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "For dataset 2, logistic regression and GDA have same performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### (g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "GDA performs poorly on dataset 1 but OK on dataset 2. The reason is that GDA assumes $p(x \\ \\vert \\ y)$ is Gaussian, whereas dataset 1 is non-Gaussian.\n",
    "\n",
    "Take a look at dataset 1, we find that all the $x_2$'s are non-negative. Take the logarithm of $x_2$ and train the classifiers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% code\n"
    }
   },
   "outputs": [],
   "source": [
    "x_train, y_train = util.load_dataset(ds1_training_set_path)\n",
    "x_valid, y_valid = util.load_dataset(ds1_valid_set_path)\n",
    "transformed_x_train = np.stack((x_train[:,0], np.log(x_train[:,1])), axis=1)\n",
    "transformed_x_valid = np.stack((x_valid[:,0], np.log(x_valid[:,1])), axis=1)\n",
    "\n",
    "# Train logistic regression\n",
    "transformed_x_train_with_intercept = util.add_intercept(transformed_x_train)\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(transformed_x_train_with_intercept, y_train)\n",
    "\n",
    "# Train GDA\n",
    "gda = GDA()\n",
    "gda.fit(transformed_x_train, y_train)\n",
    "\n",
    "# Plot decision boundaries on training set 1\n",
    "plot(transformed_x_train, y_train, theta_1=log_reg.theta, legend_1='logistic regression', theta_2=gda.theta, legend_2='GDA', title='Transformed Training Set 1')\n",
    "\n",
    "# Plot decision boundaries on validation set 1\n",
    "plot(transformed_x_valid, y_valid, theta_1=log_reg.theta, legend_1='logistic regression', theta_2=gda.theta, legend_2='GDA', title='Transformed Validation Set 1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now we can see that logistic regression and GDA have pretty much the same performance on dataset 1."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs229",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
